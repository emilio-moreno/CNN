{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0be237d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d6643986",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    '''\n",
    "    Abbreviations:\n",
    "    W : Weight matrix for each layer.\n",
    "    b : Bias vector for each layer.\n",
    "    z : Neuron vector before applying sigmoid function (unsmoothed).\n",
    "    a : Network input vector / layer activation vector.\n",
    "    y : Expected network output vector.\n",
    "\n",
    "    Notes on indeces:\n",
    "        As calculations are performed from first hidden layer\n",
    "        onward and Python is zero-indexed, we denote this first\n",
    "        non-input layer as zero.\n",
    "    '''\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        '''Calculates sigmoid of z.'''\n",
    "        return 1 / (1 + np.e**(-z))\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(z):\n",
    "        '''Evaluates derivate of sigmoid at z.'''\n",
    "        half_exponent = np.e**(z / 2)\n",
    "        return (half_exponent / (half_exponent + 1))**2\n",
    "    \n",
    "    \n",
    "    def __init__(self, size):\n",
    "        self.number_of_hidden_layers = len(size) - 2\n",
    "        self.size = size\n",
    "        self.biases = [np.random.randn(y, 1) for y in size[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(size[:-1], size[1:])]\n",
    "\n",
    "\n",
    "    def check_input(self, a):\n",
    "        '''Checks validity of input vector a.'''\n",
    "        if not isinstance(a, np.ndarray):\n",
    "            a = np.transpose(np.array(a))\n",
    "        if a.shape != (self.size[0], 1):\n",
    "            raise ValueError(f'Input vector must be of shape ({self.size[0]}, 1). Received shape {a.shape}.')\n",
    "        return a\n",
    "\n",
    "    def check_output(self, y):\n",
    "        '''Checks validity of expected output vector y.'''\n",
    "        if not isinstance(y, np.ndarray):\n",
    "            y = np.transpose(np.array(y))\n",
    "        if y.shape != (self.size[-1], 1):\n",
    "            raise ValueError(f'Output vector must be of shape ({self.size[-1]}, 1). Received shape {y.shape}.')\n",
    "        return y\n",
    "\n",
    "    \n",
    "    def front_propagate(self, a):\n",
    "        '''Front propagates input vector a.'''\n",
    "        a = self.check_input(a)\n",
    "        \n",
    "        for W, b in zip(self.weights, self.biases):\n",
    "            a = self.sigmoid(np.dot(W, a) + b)\n",
    "        return a\n",
    "\n",
    "\n",
    "    def calculate_cost(self, data):\n",
    "        '''\n",
    "        Calculates cost with respect to (input vector, expected output vector)\n",
    "        pairs in data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : array_like\n",
    "            Array containing (input vector, expected output vector) pairs.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cost : float\n",
    "            Average of evaluated cost at each data pair.\n",
    "        '''\n",
    "        cost = 0\n",
    "        data_length = len(data)\n",
    "        \n",
    "        for a, y in data:\n",
    "            y = self.check_output(y)            \n",
    "            difference = self.front_propagate(a) - y\n",
    "            cost += float(np.dot(np.transpose(difference), difference)) / data_length\n",
    "        return cost\n",
    "\n",
    "\n",
    "    ## We may return a list of unsmoothed activations and combine with\n",
    "    ## calculate neuron activations.\n",
    "    def feedforward(self, a, n):\n",
    "        '''\n",
    "        Front propagates input vector a up to (n - 1)th layer\n",
    "        then calculates z (unsmoothed neuron vector) of nth layer.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        a : array_like\n",
    "            Network input vector.\n",
    "        n : int\n",
    "            Number of layer to which propagate.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        z : Unsmoothed nth layer activations.\n",
    "        '''\n",
    "        a = self.check_input(a)\n",
    "\n",
    "        if n > 0:\n",
    "            for W, b in zip(self.weights[:n], self.biases[:n]):\n",
    "                a = self.sigmoid(np.dot(W, a) + b)\n",
    "        z = np.dot(self.weights[n], a) + self.biases[n]\n",
    "        return z\n",
    "\n",
    "    \n",
    "    def calculate_neuron_activations(self, a):\n",
    "        '''\n",
    "        Calculates neuron activations at each layer from input vector a.\n",
    "        Returns list containing 2D column vectors from each layer activations.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        a : array_like\n",
    "            Network input vector.\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        neuron_activations : list of ndarray\n",
    "            List of 2D arrays. Each 2D array is a column vector containing layer\n",
    "            activations.\n",
    "        '''\n",
    "        a = self.check_input(a)\n",
    "        \n",
    "        neuron_activations = list([a])\n",
    "        for W, b in zip(self.weights, self.biases):\n",
    "            a = np.dot(W, a) + b\n",
    "            neuron_activations.append(a)\n",
    "        return neuron_activations\n",
    "\n",
    "    \n",
    "    def calculate_output_partials(self, a, y):\n",
    "        '''\n",
    "        Calculates partials of cost function (MSE) with respect to last\n",
    "        layer unsmoothed neuron activations.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        a : array_like\n",
    "            Network input vector.\n",
    "        y : array_like\n",
    "            Expected network output.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output_error : ndarray\n",
    "            2D array, column vector. Partials of cost function with respect\n",
    "            to last layer unsmoothed neuron activations.\n",
    "        '''\n",
    "        a = self.front_propagate(a)\n",
    "        y = self.check_output(y)\n",
    "        \n",
    "        sig_derivative = self.sigmoid_derivative(a)\n",
    "        gradient_cost = a - y\n",
    "        output_partials = np.multiply(gradient_cost, sig_derivative)\n",
    "        return output_partials\n",
    "\n",
    "\n",
    "    ## Instead of feedforwarding each z inside, we may pass a list of already\n",
    "    ## calculated z.\n",
    "    def calculate_layer_partials(self, a, n, next_partials):\n",
    "        '''\n",
    "        Calculates nth layer partials from (n + 1)th layer partials.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        a : array_like\n",
    "            Network input vector.\n",
    "        n : int\n",
    "            Number of layer at which partials are calculated.\n",
    "        next_partials : ndarray\n",
    "            2D array, column vector. Cost function partials with respect to unsmoothed\n",
    "            neurons from (n + 1)th layer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        n_partials : ndarray\n",
    "            2D array, column vector. Cost function partials with respect to unsmoothed\n",
    "            neurons from nth layer.\n",
    "        '''\n",
    "        z = self.feedforward(a, n)\n",
    "        sig_derivative = self.sigmoid_derivative(z)\n",
    "        weighted_partials = np.dot(np.transpose(self.weights[n + 1]), next_partials)\n",
    "        n_partials = np.multiply(weighted_partials, sig_derivative)\n",
    "        return n_partials\n",
    "\n",
    "\n",
    "    ## Can include in back propagate direclty.\n",
    "    def calculate_weight_partials(self, n_partials, prev_a):\n",
    "        '''\n",
    "        Calculates matrix containing each of the cost function partials\n",
    "        with respects to weights on nth layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_partials : ndarray\n",
    "            2D array, column vector. Cost function partials with respect to unsmoothed\n",
    "            neurons from nth layer.\n",
    "\n",
    "        prev_ a : ndarray\n",
    "            2D array, column vector. Neuron activations from (n - 1)th layer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        n_weight_partials : ndarray\n",
    "            2D array, matrix. Cost function partials with respect to weights on\n",
    "            nth layer.\n",
    "        '''\n",
    "        n_weight_partials = np.dot(n_partials, np.transpose(prev_a))\n",
    "        return n_weight_partials\n",
    "\n",
    "    ## We can combine the two for loops inside.\n",
    "    def back_propagate(self, a, y):\n",
    "        '''\n",
    "        Calculates gradient of the cost function with respect to\n",
    "        weights and biases, evaluated at input vector a and expected\n",
    "        output vector y. Returns 3D array of column vectors of each layer\n",
    "        of bias partials and 3D array of matricies of each layer weights partials.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        a : array_like\n",
    "            Network input vector.\n",
    "        y : array_like\n",
    "            Network expected output vector.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bias_partials : list of ndarray\n",
    "            List of 2D arrays (column vectors). Partials of the cost\n",
    "            function with respect to biases. Each column vector holds\n",
    "            partials with respect to biases of ith layer.\n",
    "        weight_partials : list of ndarray\n",
    "            List of 2d ndarrays, array of 2D matrices. Partials of the cost\n",
    "            function with respect to weights. Each 2D matrix holds partials\n",
    "            with respect to weights of ith layer.\n",
    "        '''\n",
    "        output_partials = self.calculate_output_partials(a, y)\n",
    "        # Partials with respect to unsmoothed neurons in each layer.\n",
    "        z_partials = []\n",
    "        # We'll add partials in reverse order, starting from last layer\n",
    "        # and working our way to the first hidden layer.\n",
    "        z_partials.append(output_partials)\n",
    "        # As Python is zero indexed, we substract 1 from the number of hiden layers.\n",
    "        for n in range(self.number_of_hidden_layers - 1, -1, -1):\n",
    "            n_partials = self.calculate_layer_partials(a, n, z_partials[-1])\n",
    "            z_partials.append(n_partials)\n",
    "        z_partials.reverse()\n",
    "\n",
    "        # Partials with respect to unsmoothed neurons are\n",
    "        # the same as partials with respect to biases.\n",
    "        bias_partials = z_partials\n",
    "\n",
    "        # Neuron activations for each layer.\n",
    "        neuron_activations = self.calculate_neuron_activations(a)\n",
    "        weight_partials = []\n",
    "        for z_partial, neuron in zip(z_partials, neuron_activations[:-1]):\n",
    "            n_weight_partials = self.calculate_weight_partials(z_partial, neuron)\n",
    "            weight_partials.append(n_weight_partials)\n",
    "\n",
    "        return weight_partials, bias_partials\n",
    "\n",
    "\n",
    "    def update(self, a, y, learning_rate):\n",
    "        '''\n",
    "        Updates neural network weights and biases using gradient descent.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        a : array_like\n",
    "            Network input vector.\n",
    "        y : array_like\n",
    "            Network expected output vector.\n",
    "        learning_rate : float\n",
    "            Learning rate of neural network.\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "        weight_partials, bias_partials = self.back_propagate(a, y)\n",
    "        new_weights = []\n",
    "        new_biases = []\n",
    "        for W, b, W_p, b_p in zip(self.weights, self.biases, weight_partials, bias_partials):\n",
    "            new_weights.append(W - learning_rate * W_p)\n",
    "            new_biases.append(b - learning_rate * b_p)\n",
    "\n",
    "        self.weights = new_weights\n",
    "        self.biases = new_biases\n",
    "            \n",
    "\n",
    "    def SGD(self, mb_size, learning_rate, training_data, test_data = None):\n",
    "        '''\n",
    "        Applies stochastic gradient descent to neural network by using\n",
    "        batches of size mb_size. Test data may be provided to track\n",
    "        cost function after every epoch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mb_size : int\n",
    "            Size of mini-batch.\n",
    "        learning_rate : float\n",
    "            Learning rate of neural network.\n",
    "        training_data : array_like\n",
    "            Array containing (input vector, expected output vector) pairs.\n",
    "        test_data : array_like\n",
    "            Optional. Array containing (input vector, expected output vector) pairs\n",
    "            used to calculate cost function after every epoch.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        costs : ndarray\n",
    "            Array containing cost calculate from test_data after each mini-batch.\n",
    "        '''\n",
    "        np.random.shuffle(training_data)\n",
    "\n",
    "        # Number of mini-batches that can be generated from training data.\n",
    "        n_of_mb = int(np.ceil(len(training_data) / mb_size))\n",
    "        # Array to save test data costs.\n",
    "        costs = np.zeros(n_of_mb)\n",
    "        for i in range(n_of_mb):\n",
    "            for a, y in training_data[i * mb_size:(i + 1) * mb_size]:\n",
    "                # Instead of taking the average of partials and then updating,\n",
    "                # we can directly update with each partials scaled by 1 / mb_size.\n",
    "                self.update(a, y, learning_rate / mb_size)\n",
    "            if test_data != None:\n",
    "                costs[i] = self.calculate_cost(test_data)\n",
    "\n",
    "        if test_data != None:\n",
    "            return costs\n",
    "                \n",
    "\n",
    "    def print_parameters(self):\n",
    "        biases_shapes = [b.shape for b in self.biases]\n",
    "        weights_shapes = [W.shape for W in self.weights]\n",
    "        print(f'layers: {self.size}, biases: {biases_shapes}, weights: {weights_shapes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "91bc6fd8-a7b3-4c9f-8509-66a02214a7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers: [20, 300, 20], biases: [(300, 1), (20, 1)], weights: [(300, 20), (20, 300)]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "net_size = [20, 300, 20]\n",
    "net = Network(net_size)\n",
    "net.print_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "92cadb1b-b2c8-481b-a755-587d818c0f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data / test data\n",
    "# Percentage to be treated as traning_data\n",
    "ratio = 0.9\n",
    "data_size = 100000\n",
    "input_vectors = [net.sigmoid(np.random.normal(0, 1, (20, 1))) for n in range(data_size)]\n",
    "output_vectors = [np.cos(a) for a in input_vectors]\n",
    "data = []\n",
    "for a, y in zip(input_vectors, output_vectors):\n",
    "    data.append((a, y))\n",
    "np.random.shuffle(data)\n",
    "training_data = data[:int(0.9 * data_size)]\n",
    "test_data = data[int(0.9 * data_size):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bc1fb421-0178-4b09-b3ca-160c34ec4201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.460410944248395\n",
      "[0.25371292 0.14277961 0.11715329 0.11316049 0.10896235 0.10353365\n",
      " 0.10338407 0.10171091 0.10249596]\n"
     ]
    }
   ],
   "source": [
    "print(net.calculate_cost(test_data))\n",
    "mb_size = 10000\n",
    "lr = 20\n",
    "costs = net.SGD(mb_size, lr, training_data, test_data)\n",
    "print(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "52013079-394f-44bf-a251-dc92543d5697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3587889589606101"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(net.front_propagate(input_vectors[100]) - input_vectors[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a2e244c2-f869-491d-8166-dda75e8363a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  10,  100, 1000],\n",
       "       [  20,  200, 2000],\n",
       "       [  30,  300, 3000],\n",
       "       [  40,  400, 4000],\n",
       "       [  50,  500, 5000]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Tests\n",
    "B = np.transpose(np.array([[1, 2, 3, 4, 5]]))\n",
    "A = np.transpose(np.array([[10, 100, 1000]]))\n",
    "A = np.transpose(A)\n",
    "\n",
    "np.dot(B, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d24e2840-92ba-4db7-897e-beac11b23a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (1, 'A')\n",
      "1 (2, 'B')\n",
      "2 (3, 'C')\n",
      "3 (4, 'D')\n"
     ]
    }
   ],
   "source": [
    "A = [1, 2, 3, 4]\n",
    "B = [\"A\", \"B\", \"C\", \"D\"]\n",
    "for i, c in enumerate(zip(A, B)):\n",
    "    print(i, c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
